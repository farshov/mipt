{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "#conda install pytorch torchvision -c soumith"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Tensors\n",
    "=======\n",
    "\n",
    "Tensors behave almost exactly the same way in PyTorch as they do in\n",
    "Torch.\n",
    "\n",
    "Create a tensor of size (5 x 7) with uninitialized memory:\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "a = torch.FloatTensor(5, 7)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initialize a tensor randomized with a normal distribution with mean=0, var=1:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.6739,  2.7103, -1.2867,  0.1427, -0.3954,  0.9027, -1.2930],\n",
      "        [ 0.3384,  0.3245, -1.3438,  2.2275, -1.8067,  0.8235,  0.4970],\n",
      "        [ 0.2439, -0.3172, -1.5878,  1.4373,  1.0046, -0.4516,  0.7448],\n",
      "        [ 0.2582, -0.8145, -1.0207, -0.7513,  0.9949, -2.3709, -1.4159],\n",
      "        [ 0.8753, -0.7962, -0.0073,  0.2633, -0.1270,  1.1376, -0.7386]])\n",
      "torch.Size([5, 7])\n"
     ]
    }
   ],
   "source": [
    "a = torch.randn(5, 7)\n",
    "print(a)\n",
    "print(a.size())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-info\"><h4>Note</h4><p>``torch.Size`` is in fact a tuple, so it supports the same operations</p></div>\n",
    "\n",
    "Inplace / Out-of-place\n",
    "----------------------\n",
    "\n",
    "The first difference is that ALL operations on the tensor that operate\n",
    "in-place on it will have an ``_`` postfix. For example, ``add`` is the\n",
    "out-of-place version, and ``add_`` is the in-place version.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[3.5000, 3.5000, 3.5000, 3.5000, 3.5000, 3.5000, 3.5000],\n",
      "        [3.5000, 3.5000, 3.5000, 3.5000, 3.5000, 3.5000, 3.5000],\n",
      "        [3.5000, 3.5000, 3.5000, 3.5000, 3.5000, 3.5000, 3.5000],\n",
      "        [3.5000, 3.5000, 3.5000, 3.5000, 3.5000, 3.5000, 3.5000],\n",
      "        [3.5000, 3.5000, 3.5000, 3.5000, 3.5000, 3.5000, 3.5000]]) tensor([[7.5000, 7.5000, 7.5000, 7.5000, 7.5000, 7.5000, 7.5000],\n",
      "        [7.5000, 7.5000, 7.5000, 7.5000, 7.5000, 7.5000, 7.5000],\n",
      "        [7.5000, 7.5000, 7.5000, 7.5000, 7.5000, 7.5000, 7.5000],\n",
      "        [7.5000, 7.5000, 7.5000, 7.5000, 7.5000, 7.5000, 7.5000],\n",
      "        [7.5000, 7.5000, 7.5000, 7.5000, 7.5000, 7.5000, 7.5000]])\n"
     ]
    }
   ],
   "source": [
    "a.fill_(3.5)\n",
    "# a has now been filled with the value 3.5\n",
    "\n",
    "b = a.add(4.0)\n",
    "# a is still filled with 3.5\n",
    "# new tensor b is returned with values 3.5 + 4.0 = 7.5\n",
    "\n",
    "print(a, b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some operations like ``narrow`` do not have in-place versions, and\n",
    "hence, ``.narrow_`` does not exist. Similarly, some operations like\n",
    "``fill_`` do not have an out-of-place version, so ``.fill`` does not\n",
    "exist.\n",
    "\n",
    "Zero Indexing\n",
    "-------------\n",
    "\n",
    "Another difference is that Tensors are zero-indexed. (In lua, tensors are\n",
    "one-indexed)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "b = a[0, 3]  # select 1st row, 4th column from a"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tensors can be also indexed with Python's slicing\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "b = a[:, 3:5]  # selects all rows, 4th column and  5th column from a"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "No camel casing\n",
    "---------------\n",
    "\n",
    "The next small difference is that all functions are now NOT camelCase\n",
    "anymore. For example ``indexAdd`` is now called ``index_add_``\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1., 1., 1., 1., 1.],\n",
      "        [1., 1., 1., 1., 1.],\n",
      "        [1., 1., 1., 1., 1.],\n",
      "        [1., 1., 1., 1., 1.],\n",
      "        [1., 1., 1., 1., 1.]])\n"
     ]
    }
   ],
   "source": [
    "x = torch.ones(5, 5)\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 10., 100.],\n",
      "        [ 10., 100.],\n",
      "        [ 10., 100.],\n",
      "        [ 10., 100.],\n",
      "        [ 10., 100.]])\n"
     ]
    }
   ],
   "source": [
    "z = torch.Tensor(5, 2)\n",
    "z[:, 0] = 10\n",
    "z[:, 1] = 100\n",
    "print(z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[101.,   1.,   1.,   1.,  11.],\n",
      "        [101.,   1.,   1.,   1.,  11.],\n",
      "        [101.,   1.,   1.,   1.,  11.],\n",
      "        [101.,   1.,   1.,   1.,  11.],\n",
      "        [101.,   1.,   1.,   1.,  11.]])\n"
     ]
    }
   ],
   "source": [
    "x.index_add_(1, torch.LongTensor([4, 0]), z)\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Numpy Bridge\n",
    "------------\n",
    "\n",
    "Converting a torch Tensor to a numpy array and vice versa is a breeze.\n",
    "The torch Tensor and numpy array will share their underlying memory\n",
    "locations, and changing one will change the other.\n",
    "\n",
    "Converting torch Tensor to numpy Array\n",
    "^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1., 1., 1., 1., 1.])\n"
     ]
    }
   ],
   "source": [
    "a = torch.ones(5)\n",
    "print(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1. 1. 1. 1. 1.]\n"
     ]
    }
   ],
   "source": [
    "b = a.numpy()\n",
    "print(b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([2., 2., 2., 2., 2.])\n",
      "[2. 2. 2. 2. 2.]\n"
     ]
    }
   ],
   "source": [
    "a.add_(1)\n",
    "print(a)\n",
    "print(b) \t# see how the numpy array changed in value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Converting numpy Array to torch Tensor\n",
    "^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2. 2. 2. 2. 2.]\n",
      "tensor([2., 2., 2., 2., 2.], dtype=torch.float64)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "a = np.ones(5)\n",
    "b = torch.from_numpy(a)\n",
    "np.add(a, 1, out=a)\n",
    "print(a)\n",
    "print(b)  # see how changing the np array changed the torch Tensor automatically"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All the Tensors on the CPU except a CharTensor support converting to\n",
    "NumPy and back.\n",
    "\n",
    "CUDA Tensors\n",
    "------------\n",
    "\n",
    "CUDA Tensors are nice and easy in pytorch, and transfering a CUDA tensor\n",
    "from the CPU to GPU will retain its underlying type.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let us run this cell only if CUDA is available\n",
    "if torch.cuda.is_available():\n",
    "    # creates a LongTensor and transfers it\n",
    "    # to GPU as torch.cuda.LongTensor\n",
    "    a = torch.LongTensor(10).fill_(3).cuda()\n",
    "    print(type(a))\n",
    "    b = a.cpu()\n",
    "    # transfers it to CPU, back to\n",
    "    # being a torch.LongTensor"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
